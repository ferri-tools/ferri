use assert_cmd::Command;
use predicates::prelude::*;
use std::fs;
use tempfile::tempdir;

#[test]
#[ignore] // This test requires 'ollama' to be installed and running.
fn test_e2e_with_local_model() {
    let dir = tempdir().unwrap();
    let base_path = dir.path();

    // 1. `init`: Initialize the project
    Command::cargo_bin("ferri").unwrap()
        .current_dir(base_path)
        .arg("init")
        .assert()
        .success();

    // 2. `models add`: Add a local model that does not require an API key.
    // This assumes a model like 'gemma:2b' is available via Ollama.
    Command::cargo_bin("ferri").unwrap()
        .current_dir(base_path)
        .args([
            "models", "add", "local-gemma",
            "--provider", "ollama",
            "--model-name", "gemma:2b",
        ])
        .assert()
        .success();

    // 3. `ctx`: Create a dummy Python file and add it to the context
    let context_file = base_path.join("my_script.py");
    let python_code = "def my_func():\n    return 'hello'";
    fs::write(&context_file, python_code).unwrap();

    Command::cargo_bin("ferri").unwrap()
        .current_dir(base_path)
        .args(["ctx", "add", "my_script.py"])
        .assert()
        .success();

    // 4. `with --ctx -- ollama run ...`: Execute a prompt using the context.
    // The `ferri with` command will prepend the content of `my_script.py`
    // to the final prompt given to the `ollama run` command.
    let prompt = "Explain this short Python code.";
    
    Command::cargo_bin("ferri").unwrap()
        .current_dir(base_path)
        .arg("with")
        .arg("--ctx")
        .arg("--")
        .arg("ollama")
        .arg("run")
        .arg("gemma:2b") // Specify the model directly to ollama
        .arg(prompt)
        .assert()
        .success()
        .stdout(predicate::str::contains("Python").and(predicate::str::contains("function")));
}
